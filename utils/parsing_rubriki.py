import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import re

async def get_categories_from_main_page() -> Dict[str, str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã kadrovik.uz"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get("https://kadrovik.uz/", headers=headers, timeout=15) as response:
                response.raise_for_status()
                html = await response.text()

        soup = BeautifulSoup(html, 'html.parser')
        categories = {}
        
        # –ò—â–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Å—Å—ã–ª–∫–∏ "–°–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ" –∫–æ—Ç–æ—Ä—ã–µ –≤–µ–¥—É—Ç –∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        see_all_links = soup.find_all('a', string=re.compile(r'–°–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ|—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ', re.IGNORECASE))
        
        for link in see_all_links:
            href = link.get('href', '')
            if href:
                # –ù–∞—Ö–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —Ä—è–¥–æ–º —Å —Å—Å—ã–ª–∫–æ–π)
                parent = link.parent
                if parent:
                    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Ç–æ–º –∂–µ –±–ª–æ–∫–µ
                    title_elem = parent.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                    if not title_elem:
                        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
                        title_elem = parent.parent.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) if parent.parent else None
                    
                    if title_elem:
                        category_name = title_elem.get_text(strip=True)
                        full_url = href if href.startswith('http') else f"https://kadrovik.uz{href}"
                        categories[category_name] = full_url
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã —á–µ—Ä–µ–∑ –Ω–∞–≤–∏–≥–∞—Ü–∏—é
        nav_links = soup.find_all('a', href=True)
        for link in nav_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if (text and len(text) > 3 and len(text) < 50 and
                any(keyword in text.lower() for keyword in ['–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏', '–Ω–æ–≤–æ—Å—Ç–∏', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏', '—Ñ–æ—Ä–º—ã', '–∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ', '–æ–±—É—á–µ–Ω–∏–µ', '–æ—Ç–≤–µ—á–∞–µ–º']) and
                href and not href.startswith('#')):
                
                full_url = href if href.startswith('http') else f"https://kadrovik.uz{href}"
                categories[text] = full_url
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        main_categories = {
            "–ù–æ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏": "https://kadrovik.uz/recent_publications/?group=6899",
            "–ù–æ–≤–æ—Å—Ç–∏": "https://kadrovik.uz/recent_publications/?group=6899", 
            "–õ–∞–π—Ñ—Ö–∞–∫–∏ –∫–∞–¥—Ä–æ–≤–∏–∫–∞": "https://kadrovik.uz/publish/group7347_lifehack_for_kadrovik",
            "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏": "https://kadrovik.uz/services",
            "My mehnat": "https://kadrovik.uz/publish/group7318_my_mehnat_uz_k4",
            "–ü—Ä–∏–µ–º –Ω–∞ —Ä–∞–±–æ—Ç—É": "https://kadrovik.uz/publish/group6525_priem_na_rabotu112",
            "–û—Ç–ø—É—Å–∫–∞ –∏ –æ—Ç–≥—É–ª—ã": "https://kadrovik.uz/publish/group6566_6"
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏
        categories.update(main_categories)
        
        return categories
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
        return {}

async def fetch_rubrika_articles(rubrika_url: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ä—É–±—Ä–∏–∫–∏"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(rubrika_url, headers=headers, timeout=15) as response:
                response.raise_for_status()
                html = await response.text()

        soup = BeautifulSoup(html, 'html.parser')
        articles = []

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        if rubrika_url == "https://kadrovik.uz/" or rubrika_url == "https://kadrovik.uz":
            # –ù–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏—â–µ–º —Å—Ç–∞—Ç—å–∏ –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            text_content = soup.get_text()
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                title_text = link.get_text(strip=True)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
                if (href and title_text and 
                    len(title_text) > 30 and  # –î–ª–∏–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–∞—Ç–µ–π
                    '/publish/' in href and
                    not any(skip in title_text.lower() for skip in ['—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '–ø–æ–∫–∞–∑–∞—Ç—å'])):
                    
                    full_url = href if href.startswith('http') else f"https://kadrovik.uz{href}"
                    
                    articles.append({
                        'title': title_text,
                        'url': full_url,
                        'date': datetime.now().isoformat()
                    })
                    
                    if len(articles) >= 10:
                        break
            
            return articles
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç–∞—Ç–µ–π
        possible_selectors = [
            'div.publication-item',
            'div.post-item', 
            'article',
            'div[class*="article"]',
            'div[class*="post"]',
            'div[class*="publication"]',
            '.content-item',
            '.news-item',
            '.item',
            '[class*="item"]'
        ]
        
        articles_found = []
        
        for selector in possible_selectors:
            articles_found = soup.select(selector)
            if articles_found:
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –∏—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        if not articles_found:
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                title_text = link.get_text(strip=True)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
                if (href and title_text and 
                    len(title_text) > 20 and
                    '/publish/' in href and
                    not any(skip in href.lower() for skip in ['javascript:', 'mailto:', '#', 'tel:']) and
                    not any(skip in title_text.lower() for skip in ['—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '—á–∏—Ç–∞—Ç—å', '–≥–ª–∞–≤–Ω–∞—è', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–ø–æ–∫–∞–∑–∞—Ç—å'])):
                    
                    full_url = href if href.startswith('http') else f"https://kadrovik.uz{href}"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –¥–æ–±–∞–≤–ª—è–ª–∏ –ª–∏ —É–∂–µ —ç—Ç—É —Å—Ç–∞—Ç—å—é
                    if not any(art['url'] == full_url for art in articles):
                        articles.append({
                            'title': title_text,
                            'url': full_url,
                            'date': datetime.now().isoformat()
                        })
                        
                        if len(articles) >= 10:
                            break
        else:
            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            for item in articles_found[:15]:
                title_elem = item.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']) or item.find('a')
                link_elem = item.find('a', href=True)
                
                if title_elem and link_elem:
                    title = title_elem.get_text(strip=True)
                    href = link_elem.get('href', '')
                    
                    if title and href and len(title) > 10:
                        full_url = href if href.startswith('http') else f"https://kadrovik.uz{href}"
                        
                        articles.append({
                            'title': title,
                            'url': full_url,
                            'date': datetime.now().isoformat()
                        })
                        
                        if len(articles) >= 10:
                            break

        return articles[:10]

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ä—É–±—Ä–∏–∫–∏ {rubrika_url}: {e}")
        return []

async def fetch_article_content(url: str) -> str:
    """–ü–∞—Ä—Å–∏—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=15) as response:
                response.raise_for_status()
                html = await response.text()

        soup = BeautifulSoup(html, 'html.parser')
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
        title = ""
        title_selectors = ['h1', 'h2.title', '.article-title', '.post-title', '.content-title']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                break
        
        if not title:
            title = "–°—Ç–∞—Ç—å—è —Å —Å–∞–π—Ç–∞ kadrovik.uz"
        
        # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        date = ""
        date_selectors = ['time', '.date', '.published-date', '[datetime]']
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date = date_elem.get('datetime') or date_elem.get_text(strip=True)
                break
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content_selectors = [
            '.article-content',
            '.post-content', 
            '.content',
            'main',
            '.main-content',
            '#content',
            '.text-content'
        ]
        
        content_block = None
        for selector in content_selectors:
            content_block = soup.select_one(selector)
            if content_block:
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç, –±–µ—Ä–µ–º body, –∏—Å–∫–ª—é—á–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—é
        if not content_block:
            content_block = soup.find('body')
            if content_block:
                # –£–¥–∞–ª—è–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for unwanted in content_block.find_all(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                    unwanted.decompose()
        
        if not content_block:
            return f"üì∞ {title}\nüìÖ {date}\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏."

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content_parts = [f"üì∞ {title}"]
        if date:
            content_parts.append(f"üìÖ {date}")
        content_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        text_elements = content_block.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'div'])
        
        processed_texts = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        
        for element in text_elements:
            text = element.get_text(' ', strip=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–∫—Å—Ç—ã
            if (text and len(text) > 20 and 
                text not in processed_texts and
                not any(skip in text.lower() for skip in ['javascript', 'loading', 'menu', '–Ω–∞–≤–∏–≥–∞—Ü–∏—è', '–≤–æ–π—Ç–∏', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è'])):
                
                processed_texts.add(text)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —ç–ª–µ–º–µ–Ω—Ç–∞
                if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    content_parts.append(f"\nüî∏ {text}\n")
                elif element.name == 'li':
                    content_parts.append(f"‚Ä¢ {text}")
                else:
                    content_parts.append(text)
        
        result = '\n'.join(content_parts)
        
        # –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±
        if len(result) < 200:
            all_text = content_block.get_text(' ', strip=True)
            if len(all_text) > 100:
                result = f"üì∞ {title}\nüìÖ {date}\n\n{all_text[:2000]}..."
        
        return result

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
        return f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏. –û—à–∏–±–∫–∞: {str(e)}"

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö —Ä—É–±—Ä–∏–∫ —Å —Å–∞–π—Ç–∞
async def get_all_categories():
    """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    categories = await get_categories_from_main_page()
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ
    if not categories:
        categories = {
            "–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞": "https://kadrovik.uz/",
            "–ù–æ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏": "https://kadrovik.uz/recent_publications/?group=6899",
            "–õ–∞–π—Ñ—Ö–∞–∫–∏ –∫–∞–¥—Ä–æ–≤–∏–∫–∞": "https://kadrovik.uz/publish/group7347_lifehack_for_kadrovik",
            "My mehnat": "https://kadrovik.uz/publish/group7318_my_mehnat_uz_k4",
            "–ü—Ä–∏–µ–º –Ω–∞ —Ä–∞–±–æ—Ç—É": "https://kadrovik.uz/publish/group6525_priem_na_rabotu112",
            "–û—Ç–ø—É—Å–∫–∞ –∏ –æ—Ç–≥—É–ª—ã": "https://kadrovik.uz/publish/group6566_6",
            "–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏": "https://kadrovik.uz/services"
        }
    
    return categories

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞
async def test_parser():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ kadrovik.uz...")
    
    # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —Å–∞–π—Ç–∞
    print("\nüìã –ü–æ–ª—É—á–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    categories = await get_all_categories()
    
    if categories:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
        for name, url in categories.items():
            print(f"  ‚Ä¢ {name}: {url}")
    else:
        print("‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π –∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for category_name, category_url in list(categories.items())[:5]:  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        print(f"\nüìÇ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_name}")
        articles = await fetch_rubrika_articles(category_url)
        
        if articles:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π")
            for i, article in enumerate(articles[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"  {i}. {article['title'][:80]}...")
                print(f"     URL: {article['url']}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–µ—Ä–≤–æ–π —Å—Ç–∞—Ç—å–∏
            if articles:
                print(f"\nüìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–µ—Ä–≤–æ–π —Å—Ç–∞—Ç—å–∏...")
                content = await fetch_article_content(articles[0]['url'])
                print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                print(f"–ü—Ä–µ–≤—å—é: {content[:200]}...")
        else:
            print("‚ùå –°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    print("\nüèÅ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
async def parse_category_by_name(category_name: str):
    """–ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –µ—ë –Ω–∞–∑–≤–∞–Ω–∏—é"""
    categories = await get_all_categories()
    
    if category_name in categories:
        print(f"üìÇ –ü–∞—Ä—Å–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_name}")
        articles = await fetch_rubrika_articles(categories[category_name])
        return articles
    else:
        print(f"‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è '{category_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for name in categories.keys():
            print(f"  ‚Ä¢ {name}")
        return []

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è):
# import asyncio
# asyncio.run(test_parser())